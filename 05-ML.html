
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>From Nonparametrics to Machine Learning &#8212; A Primer on Economic Data Science</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Prediction-Oriented Algorithms" href="06-ML2.html" />
    <link rel="prev" title="Numerical Optimization" href="04-optimization.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">A Primer on Economic Data Science</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Preface
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01-basic_R.html">
   Basic R
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02-advanced_R.html">
   Advanced R
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03-simulation.html">
   Simulation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03-integration.html">
   Integration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04-optimization.html">
   Numerical Optimization
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   From Nonparametrics to Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06-ML2.html">
   Prediction-Oriented Algorithms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09-data_work.html">
   Data Processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10-git.html">
   Git
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/05-ML.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/zhentaoshi/econ5170"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/zhentaoshi/econ5170/issues/new?title=Issue%20on%20page%20%2F05-ML.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/zhentaoshi/econ5170/master?urlpath=tree/docs/05-ML.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nonparametric-estimation">
   Nonparametric Estimation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-splitting">
   Data Splitting
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-validation">
     Cross Validation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variable-selection-and-prediction">
   Variable Selection and Prediction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#shrinkage-estimation-in-econometrics">
   Shrinkage Estimation in Econometrics
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#empirical-applications">
   Empirical Applications
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reading">
   Reading
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#appendix">
   Appendix
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>From Nonparametrics to Machine Learning</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nonparametric-estimation">
   Nonparametric Estimation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-splitting">
   Data Splitting
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-validation">
     Cross Validation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variable-selection-and-prediction">
   Variable Selection and Prediction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#shrinkage-estimation-in-econometrics">
   Shrinkage Estimation in Econometrics
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#empirical-applications">
   Empirical Applications
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reading">
   Reading
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#appendix">
   Appendix
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="from-nonparametrics-to-machine-learning">
<h1>From Nonparametrics to Machine Learning<a class="headerlink" href="#from-nonparametrics-to-machine-learning" title="Permalink to this headline">¶</a></h1>
<p>Machine learning has quickly grown into a big field, with applications from
scientific research to daily life.
An authoritative reference is &#64;friedman2001elements,
written at the entry-year postgraduate level.
The ideas in machine learning are general and applicable to economic investigation.
&#64;athey2018impact discusses the impact of machine learning techniques
to economic analysis.
&#64;mullainathan2017machine survey a few new commonly used methods and
demonstrate them in a real example.
&#64;taddy2018technological introduces new technology <em>artificial intelligence</em>
and the implication of the underlying economic modeling.</p>
<p>The two broad classes of machine learning methods are <em>supervised learning</em> and <em>unsupervised learning</em>.
Roughly speaking, the former is about the connection between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, while the latter
is only about <span class="math notranslate nohighlight">\(X\)</span>. Instances of the former are various regression
and classification methods; those of the latter are density estimation,
principal component analysis, and clustering.
These examples are all familiar econometric problems.</p>
<p>From an econometrician’s view, supervised machine learning is a set of data fitting procedures that focus on out-of-sample prediction.
The simplest illustration is in the regression context.
We repeat a scientific experiment for <span class="math notranslate nohighlight">\(n\)</span> times, and we harvest a dataset <span class="math notranslate nohighlight">\((y_i, x_i)_{i=1}^n\)</span>.
What would be the best way to predict <span class="math notranslate nohighlight">\(y_{n+1}\)</span> from the same experiment if we know <span class="math notranslate nohighlight">\(x_{n+1}\)</span>?</p>
<p>Machine learning is a paradigm shift against conventional statistics.
When a statistician propose a new estimator, the standard practice is to pursue
three desirable properties one after another.
We first establish its consistency, which is seen as the bottom line.
Given consistency, we want to show its asymptotic distribution. Ideally, the asymptotic
distribution is normal. Asymptotic normality is desirable as it holds for many regular estimators
and the inferential procedure is familiar to applied researchers.
Furthermore, for an asymptotically normal estimator, we want to
show efficiency, an optimality property. An efficient estimator achieves the smallest asymptotic variance
in a class of asymptotically normal estimators.</p>
<p>In addition, econometrician also cares about
model identification and economic interpretation of the empirical results.
Econometrics workflow interacts the data at hand and the model of interest. At the population level,
we think about the problem of identification. Once the parameter of interest is identified,
then we can proceed to parameter estimation and inference.
Finally, we interpret the results and hopefully they shed light on economics.</p>
<!-- \begin{figure} -->
<!-- \centering -->
<!-- \begin{tikzpicture}[node distance=5mm] -->
<!--     %\draw[help lines] (0,0) grid (9,7); -->
<!-- % Nodes -->
<!--     \node [draw, ellipse] (a) at (2,6) {Model}; -->
<!--     \node [draw, ellipse] (b) at (8,6) {abstract data}; -->
<!--     \node [color=black!40](c) at (1,5) {Population}; -->
<!--     \node (d) at (5,4) {Estimation}; -->
<!--     \node [color=black!40](e) at (1,2) {Sample}; -->
<!--     \node (f) at (5,2) {Inference}; -->
<!--     \node (g) at (5,0) {Interpretation}; -->
<!--     \node [color=black!40](h) at (1,0) {Economics}; -->
<!-- % Arrows   -->
<!--     \draw[-latex] (a) to[bend right=10] node[above, yshift=2mm] {Identification}  (b); -->
<!--     \draw[-latex] (b) to[bend right=10]  (a); -->
<!--     \draw[-latex] (a) to[bend left=0]  (d); -->
<!--     \draw[-latex] (b) to[bend right=0]  (d); -->
<!--     \draw[-latex] (d) to[bend right=0]  (f); -->
<!--     \draw[-latex] (f) to[bend right=0]  (g); -->
<!-- % Lines -->
<!--     \draw[color=black!40,very thick] (0,1) -- (2,1); -->
<!--     \draw[color=black!40,very thick] (0,4) -- (2,4); -->
<!-- \end{tikzpicture} -->
<!-- \caption{Econometrics workflow} -->
<!-- \end{figure} -->
<p>Machine learning deviates from such routines.
First, they argue efficiency is not crucial because the dataset itself is big enough so that the variance is usually small.
Second, in many situations statistical inference is not the goal, so
inferential procedure is not of interest. For example, the recommendation system on Amazon or Taobao has
a machine learning algorithm behind it. There we care about the prediction accuracy, not the causal link
why a consumer interested in one good is likely to purchase another good. Third, the world is so complex
that we have little idea about how the data is generated. We do not have to assume a data generating process (DGP).
If there is no DGP, we lose the standing ground to talk about consistency. Where would my estimator converge
to if there is no “true parameter”? With these arguments, the paradigm of conventional statistics is
smashed. In the context of econometrics,
such argument completely rejects the structural modeling tradition (the Cowles approach).</p>
<p>Readers interested in the debate are referred to &#64;breiman2001statistical.
In this lecture, we put aside the ongoing philosophical debate. Instead,  we study
the most popular machine learning methods that have found growing popularity in economics.</p>
<div class="section" id="nonparametric-estimation">
<h2>Nonparametric Estimation<a class="headerlink" href="#nonparametric-estimation" title="Permalink to this headline">¶</a></h2>
<p><em>Parametric</em> is referred to problems with a finite number of parameters,
whereas <em>nonparametric</em> is associated with an infinite number of parameters.
Nonparametric estimation is nothing new to statisticians. However, some ideas
in this old topic is directly related to the underlying principles of machine learning methods.</p>
<p>Consider the density estimation given a sample <span class="math notranslate nohighlight">\((x_1,\ldots,x_n)\)</span>. If we assume that
the sample is drawn from a parametric family, for example the normal distribution, then we can use the
maximum likelihood estimation to learn the mean and the variance.
Nevertheless, when the parametric family is misspecified, the MLE estimation
is inconsistent in theory, and we can at best identify a <em>pseudo true value</em>.
In practice, what is the correct parametric family is unknown.
If we do not want to impose a parametric assumption, then in principle
we will have to use an infinite number of parameters to fully characterize the density.
One well-known nonparametric estimation is the histogram. The shape of the bars of the
histogram depends on the partition of the support. If the grid system on the support is too
fine, then each bin will have only a few observations. Despite small bias, the estimation
will suffer a large variance. On the other hand, if the grid system is too coarse, then each bin will be wide. It causes big bias, though the variance is small because each bin contains many observations. There is an bias-variance tradeoff. This tradeoff is the defining feature not only for nonparametric estimation but for all machine learning methods.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">&lt;-</span> <span class="m">200</span>

<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">3</span><span class="p">,</span> <span class="m">3</span><span class="p">))</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mar</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">1</span><span class="p">))</span>

<span class="n">x_base</span> <span class="o">&lt;-</span> <span class="nf">seq</span><span class="p">(</span><span class="m">0.01</span><span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="n">by</span> <span class="o">=</span> <span class="m">0.01</span><span class="p">)</span>
<span class="n">breaks_list</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">4</span><span class="p">,</span> <span class="m">12</span><span class="p">,</span> <span class="m">60</span><span class="p">)</span>

<span class="nf">for </span><span class="p">(</span><span class="n">ii</span> <span class="n">in</span> <span class="m">1</span><span class="o">:</span><span class="m">3</span><span class="p">){</span>
  <span class="n">x</span> <span class="o">&lt;-</span> <span class="nf">rbeta</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">2</span><span class="p">)</span> <span class="c1"># beta distribution</span>
  <span class="nf">for </span><span class="p">(</span> <span class="n">bb</span> <span class="n">in</span> <span class="n">breaks_list</span><span class="p">){</span>
    <span class="nf">hist</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">breaks</span> <span class="o">=</span> <span class="n">bb</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;&quot;</span><span class="p">,</span> <span class="n">freq</span> <span class="o">=</span> <span class="kc">FALSE</span><span class="p">,</span> <span class="n">ylim</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">3</span><span class="p">),</span><span class="n">xlim</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">))</span>
    <span class="nf">lines</span><span class="p">(</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">dbeta</span><span class="p">(</span> <span class="n">x_base</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">2</span><span class="p">),</span> <span class="n">x</span> <span class="o">=</span> <span class="n">x_base</span> <span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;red&quot;</span> <span class="p">)</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>\begin{figure}
\centering
\begin{tikzpicture}[scale=1, transform shape] %size of the picture</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\draw[-&gt;, thick] (0,0) node[below left]{0} to (12,0) node[below]{bin size};
\draw[-&gt;, thick] (0,0) to (0,8);
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\draw [color = red, thick] (2,7) .. controls (6,2) and (10,5) .. (10.5,6.5) node[right]{MSE}; % MSE
\draw (1,0.1)     node[above]{bias$^2$}   parabola (11,5); % bias^2
\draw (11,1)    node[above]{variance}   parabola (1,7); % variance
</pre></div>
</div>
<p>\end{tikzpicture}
\caption{Bias-variance tradeoff}
\end{figure}</p>
<p>Another example of nonparametric estimation is  the conditional mean
<span class="math notranslate nohighlight">\(f(x) = E[y_i |x_i = x]\)</span> given a sample <span class="math notranslate nohighlight">\((y_i, x_i)\)</span>. This is what we encountered in the first lecture of graduate econometrics Econ5121A. We solve the minimization problem</p>
<div class="math notranslate nohighlight">
\[
\min_f E[ (y_i - f(x_i) )^2 ]
\]</div>
<p>In Econ5121A, we use the linear projection to approximate <span class="math notranslate nohighlight">\(f(x)\)</span>.
But the conditional mean is in general a nonlinear function.
If we do not know the underlying parametric estimation of
<span class="math notranslate nohighlight">\((y_i,x_i)\)</span>, estimating <span class="math notranslate nohighlight">\(f(x)\)</span> becomes a non-parametric problem.
In practice, the sample size <span class="math notranslate nohighlight">\(n\)</span> is always finite. The sample minimization problem is</p>
<div class="math notranslate nohighlight">
\[
\min_f \sum_{i=1}^n (y_i - f(x_i) )^2.
\]</div>
<p>We still have to restrict the class of functions that we search for the minimizer.
If we assume <span class="math notranslate nohighlight">\(f\)</span> is a continuous function, one way to estimate it is the kernel method based on density
estimation.</p>
<p>An alternative is to use a series expansion to approximate the function.
Series expansion generates many additive regressors whose coefficients will be
estimated. This is one way to “create” many variables on the right-hand side of a
linear regression.
For example, any bounded, continuous and differentiate function has a series
representation <span class="math notranslate nohighlight">\(f(x) = \sum_{k=0}^{\infty} \beta_k \cos (\frac{k}{2}\pi x )\)</span>. In finite sample,
we choose a finite <span class="math notranslate nohighlight">\(K\)</span>, usually much smaller than <span class="math notranslate nohighlight">\(n\)</span>, as a cut-off.
Asymptotically <span class="math notranslate nohighlight">\(K \to \infty\)</span> as <span class="math notranslate nohighlight">\(n \to \infty\)</span> so that</p>
<div class="math notranslate nohighlight">
\[
f_K(x) = \sum_{k=0}^{K} \beta_k \cos \left(\frac{k}{2}\pi x \right) \to f(x).
\]</div>
<p>Similar bias-variance tradeoff appears in this nonparametric regression.
If <span class="math notranslate nohighlight">\(K\)</span> is too big, <span class="math notranslate nohighlight">\(f\)</span> will be too flexible and it can achieve <span class="math notranslate nohighlight">\(100\%\)</span> of in-sample R-squared.
This is not useful for out-of-sample prediction. Such prediction will have large variance, but small bias.
On the other extreme, a very small <span class="math notranslate nohighlight">\(K\)</span> will make <span class="math notranslate nohighlight">\(f_K(x)\)</span> too rigid to approximate general nonlinear functions. It causes large bias but small variance.</p>
<p>The fundamental statistical mechanism that governs the performance is the bias-variance
tradeoff. Thus we need <em>regularization</em> to balance the two components in the mean-squared
error. Choosing the bandwidth is one way of regularization, choosing the terms of series
expansion is another way of regularization.</p>
<p>A third way of regularization is to specify a
sufficiently large <span class="math notranslate nohighlight">\(K\)</span>, and then add a penalty term to control the complexity of the additive series. The optimization problem is</p>
<div class="math notranslate nohighlight">
\[
\min_\beta \  \frac{1}{2n}  \sum_{i=1}^n \left(y_i - \sum_{k=0}^{K} \beta_k f_k(x_i) \right)^2
+ \lambda \sum_{k=0}^K \beta_k^2,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> is the tuning parameter such that <span class="math notranslate nohighlight">\(\lambda \to 0\)</span> as <span class="math notranslate nohighlight">\(n\to \infty\)</span>, and
<span class="math notranslate nohighlight">\(f_k(x_i) = \cos \left(\frac{k}{2}\pi x_i \right)\)</span>. In compact notation, let <span class="math notranslate nohighlight">\(y=(y_1,\ldots,y_n)'\)</span> and
<span class="math notranslate nohighlight">\(X = (X_{ik} = f_k(x_i) )\)</span>, the above problem can be written as</p>
<div class="math notranslate nohighlight">
\[
(2n)^{-1} (Y-X\beta)'(Y-X\beta) + \lambda \Vert \beta \Vert_2 ^2,
\]</div>
<p>and this optimization has an explicit solution <span class="math notranslate nohighlight">\(\hat{\beta} = (X'X+\lambda I)^{-1} X'Y\)</span>. This is the <em>ridge regression</em> proposed in 1970’s.
This penalization scheme is very similar
to what we will discuss in the next section in variable selection.</p>
<p>The practical question is, given a regularization problem, how to choose the tuning
parameter? This is a difficult statistical problem with active research. The main
theoretical proposal is either using an <em>information criterion</em>
(for example, Akaike information criterion <span class="math notranslate nohighlight">\(\log\hat{\sigma}^2 + 2K\)</span> or Bayesian information criterion <span class="math notranslate nohighlight">\(\log\hat{\sigma}^2 + K\log n\)</span> ), or <em>cross validation</em>.</p>
</div>
<div class="section" id="data-splitting">
<h2>Data Splitting<a class="headerlink" href="#data-splitting" title="Permalink to this headline">¶</a></h2>
<!-- \begin{figure} -->
<!-- \centering -->
<!-- \begin{tikzpicture}[node distance = 10 mm, thick, scale=1, transform shape] -->
<!-- % Nodes -->
<!--   \node[ellipse, draw] -->
<!--     (a) [label=above left:$data$]{Training Data}; -->
<!--   \node[ellipse, draw, right = of a] -->
<!--     (b) {Validation Data}; -->
<!--   \node[ellipse, draw=red,fill=black!0, right = of b, text width=3cm, align=center] -->
<!--     (c) {(Out of Sample) Testing Data}; -->
<!--   \node[ellipse, draw=blue,fill=black!0, below right = 20 mm of a] -->
<!--     (d) {Fitted model} ; -->
<!--   \node[ellipse, draw=blue,fill=black!0, right= 10 mm of d, text width=3.5cm, align=center] -->
<!--     (e) {Best tuning Parameter (Model)}; -->
<!--   \node[ellipse, draw, left = 20 mm of d, loosely dashed] -->
<!--     (f) -->
<!--     [label = below: \textcolor{black!40}{Many Sets of Tuning Parameters}] -->
<!--     {Model}; -->
<!-- % Arrows -->
<!--     \draw [->, black] (a) -- (d); -->
<!--     \draw [->, blue] (d) -- (b); -->
<!--     \draw [->, blue] (b) -- (e); -->
<!--     \draw [->, blue] (e) -- (c); -->
<!--     \draw [->, black, loosely dashed] (f) -- (d); -->
<!-- % Caption     -->
<!--   \node[below = of d] { -->
<!--         \begin{tabular}{l} -->
<!--             $\bullet$ Data splitting can be done by cross validation \\ -->
<!--             $\bullet$ A data driven approach for feature selection -->
<!--         \end{tabular}}; -->
<!-- \end{tikzpicture} -->
<!-- \caption{Learning workflow} -->
<!-- \end{figure} -->
<p>The workflow of machine learning methods is quite different from econometrics. The main purpose is often prediction instead of interpretation.
They use some “off-the-shelf” generic learning methods, and
the models are measured by their performance in prediction.
In order to avoid overfitting it is essential to tune at least a few tuning parameters.</p>
<p>Most machine learning methods take an agnostic view about the DGP, and they explicitly
acknowledge model uncertainty. To address the issue of model selection (tuning parameter selection),
in a data rich environment we split the data into three parts. A <em>training dataset</em>
is used to fit the model parameter given the tuning parameters. A <em>validation dataset</em> is
used to compare the out-of-sample performance under different tuning parameters.
It helps decide a set of desirable tuning parameters. Ideally, the <em>testing sample</em> should be
kept by a third party away from the modeler. The testing sample is the final
judge of the relative merit of the fitted models.</p>
<p>The R package <code class="docutils literal notranslate"><span class="pre">caret</span></code> (Classification And REgression Training) provides a framework for many machine learning methods.
The function <a class="reference external" href="https://topepo.github.io/caret/data-splitting.html"><code class="docutils literal notranslate"><span class="pre">createDataPartition</span></code></a>
splits the sample for both cross-sectional data and time series.</p>
<div class="section" id="cross-validation">
<h3>Cross Validation<a class="headerlink" href="#cross-validation" title="Permalink to this headline">¶</a></h3>
<p>An <span class="math notranslate nohighlight">\(S\)</span>-fold cross validation partitions the dataset into <span class="math notranslate nohighlight">\(S\)</span> disjoint sections. In each iteration, it picks one of the sections as the validation sample and the other <span class="math notranslate nohighlight">\(S-1\)</span> sections as the training sample, and computes an out-of-sample goodness-of-fit measurement, for example <em>mean-squared prediction error</em> <span class="math notranslate nohighlight">\({n_v}^{-1} \sum_{i \in val} (y_i - \hat{y}_i)^2\)</span> where <span class="math notranslate nohighlight">\(val\)</span> is the validation set and <span class="math notranslate nohighlight">\(n_v\)</span> is its cardinality,  or <em>mean-absolute prediction error</em> <span class="math notranslate nohighlight">\({n_v}^{-1}\sum_{i \in val} |y_i - \hat{y}_i|\)</span>. Repeat this process for <span class="math notranslate nohighlight">\(S\)</span> times so that each of the <span class="math notranslate nohighlight">\(S\)</span> sections are treated as the validation sample, and average the goodness-of-fit measurement over the <span class="math notranslate nohighlight">\(S\)</span> sections to determined the best tuning parameter. If <span class="math notranslate nohighlight">\(S=n-1\)</span>, it is called <em>leave-one-out cross validation</em>, but it can be computationally too expensive when  <span class="math notranslate nohighlight">\(n\)</span> is big. Instead, in practice we can  <span class="math notranslate nohighlight">\(S=5\)</span> for 10, called 5-fold cross validation or 10-fold cross validation, respectively.</p>
<p>\begin{figure}
\centering
\includegraphics[width = 14cm]{graph/CV_Figure}
\caption{Rolling window time series cross validation}
\end{figure}</p>
<p>In time series context, cross validation must preserve the dependence structure. If the time series is stationary, we can partition the data into <span class="math notranslate nohighlight">\(S\)</span> consecutive blocks. If the purpose is ahead-of-time forecasting, then we can use nested CV. The figure shows a nested CV with fixed-length rolling window scheme, while the sub-training data can also be an extending rolling window.</p>
</div>
</div>
<div class="section" id="variable-selection-and-prediction">
<h2>Variable Selection and Prediction<a class="headerlink" href="#variable-selection-and-prediction" title="Permalink to this headline">¶</a></h2>
<p>In modern scientific analysis, the number of covariates <span class="math notranslate nohighlight">\(x_i\)</span> can be enormous.
In DNA microarray analysis, we look for association between a symptom and genes.
Theory in biology indicates that only a small handful of genes are involved,
but it does not pinpoint which ones are the culprits.
Variable selection is useful to identify the relevant genes, and then we can
think about how to edit the genes to prevent certain diseases and better people’s life.</p>
<p>Explanatory variables are abundant in some empirical economic examples.
For instance, a questionnaire from the <a class="reference external" href="https://discover.ukdataservice.ac.uk/series/?sn=2000028">UK Living Costs and Food Survey</a>, a survey
widely used for analysis of demand theory and family consumption,
consists of thousand of questions.
&#64;giannone2017economic <a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3031893">link</a> experiment variable selection methods in 6 widely used economic datasets with many predictors.</p>
<p><strong>Hazard of model selection</strong> To elaborate the distortion of test size when the <span class="math notranslate nohighlight">\(t\)</span> statistic is selected from two models in pursuit of significance.
$<span class="math notranslate nohighlight">\(
\begin{pmatrix}y\\
x_{1}\\
x_{2}
\end{pmatrix}\sim N\left(0,\begin{pmatrix}1 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; \sqrt{0.5}\\
0 &amp; \sqrt{0.5} &amp; 1
\end{pmatrix}\right)
\)</span><span class="math notranslate nohighlight">\(
Both \)</span>x_1<span class="math notranslate nohighlight">\( and \)</span>x_2<span class="math notranslate nohighlight">\( are independent of \)</span>y$. The test size dependens on the correlation between the two regressors.
If the test is conducted for a single model, the size is the pre-specified 10%.
If we try two models, the size is inflated to about 17%.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">&lt;-</span> <span class="m">100</span>
<span class="n">Rep</span> <span class="o">&lt;-</span> <span class="m">5000</span>

<span class="n">t_stat</span> <span class="o">&lt;-</span> <span class="nf">function</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">beta_hat</span> <span class="o">&lt;-</span> <span class="nf">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="nf">sum</span><span class="p">(</span><span class="n">x</span><span class="o">^</span><span class="m">2</span><span class="p">)</span>
  <span class="n">e_hat</span> <span class="o">&lt;-</span> <span class="n">y</span> <span class="o">-</span> <span class="n">beta_hat</span> <span class="o">*</span> <span class="n">x</span>
  <span class="n">sigma2_hat</span> <span class="o">&lt;-</span> <span class="nf">var</span><span class="p">(</span><span class="n">e_hat</span><span class="p">)</span>
  <span class="n">t_stat</span> <span class="o">&lt;-</span> <span class="n">beta_hat</span> <span class="o">/</span> <span class="nf">sqrt</span><span class="p">(</span><span class="n">sigma2_hat</span> <span class="o">/</span> <span class="nf">sum</span><span class="p">(</span><span class="n">x</span><span class="o">^</span><span class="m">2</span><span class="p">))</span>
  <span class="nf">return</span><span class="p">(</span><span class="n">t_stat</span><span class="p">)</span>
<span class="p">}</span>

<span class="n">res</span> <span class="o">&lt;-</span> <span class="nf">matrix</span><span class="p">(</span><span class="kc">NA</span><span class="p">,</span> <span class="n">Rep</span><span class="p">,</span> <span class="m">2</span><span class="p">)</span>

<span class="nf">for </span><span class="p">(</span><span class="n">r</span> <span class="n">in</span> <span class="m">1</span><span class="o">:</span><span class="n">Rep</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">y</span> <span class="o">&lt;-</span> <span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
  <span class="n">x1</span> <span class="o">&lt;-</span> <span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
  <span class="n">x2</span> <span class="o">&lt;-</span> <span class="nf">sqrt</span><span class="p">(</span><span class="m">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">+</span> <span class="nf">sqrt</span><span class="p">(</span><span class="m">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

  <span class="n">res</span><span class="p">[</span><span class="n">r</span><span class="p">,</span> <span class="p">]</span> <span class="o">&lt;-</span> <span class="nf">c</span><span class="p">(</span><span class="nf">t_stat</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x1</span><span class="p">),</span> <span class="nf">t_stat</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x2</span><span class="p">))</span>
<span class="p">}</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">mean</span><span class="p">(</span><span class="nf">apply</span><span class="p">(</span><span class="nf">abs</span><span class="p">(</span><span class="n">res</span><span class="p">),</span> <span class="m">1</span><span class="p">,</span> <span class="n">max</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nf">qnorm</span><span class="p">(</span><span class="m">0.95</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<p>Conventionally, applied economists do not appreciate the problem of variable selection, even though they always select variables implicitly. They rely on their prior knowledge to
choose variables from a large number of potential candidates.
Recently years economists wake up from the long lasting negligence.
&#64;stock2012generalized are concerning about forecasting 143 US macroeconomic indicators.
They conduct a horse race of several variable selection methods.</p>
<p>The most well-known variable selection method in regression context is the least-absolute-shrinkage-and-selection-operator
(Lasso) [&#64;tibshirani1996regression].
Upon the usual OLS criterion function, Lasso penalizes the <span class="math notranslate nohighlight">\(L_1\)</span> norm of the coefficients.
The criterion function of Lasso is written as</p>
<div class="math notranslate nohighlight">
\[
(2n)^{-1} (Y-X\beta)'(Y-X\beta) + \lambda \Vert \beta \Vert_1
\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda \geq 0\)</span> is a tuning parameter. Unlike OLS or ridge regression, Lasso does not have a closed-form solution. Fortunately, it is an convex optimization so numerical optimization is fast and reliable for high-dimensional parameter.</p>
<p>In a wide range of values of <span class="math notranslate nohighlight">\(\lambda\)</span>,
Lasso can shrink some coefficients exactly to 0, which suggests that these variables are likely to be
irrelevant in the regression. This phenomenon is similar to “corner solution” that we solve utility maximization in microeconomics.</p>
<p>In terms of theoretical property, &#64;zou2006adaptive finds that Lasso cannot consistently
distinguish the relevant variables from the irrelevant ones.</p>
<p><img alt="lasso" src="_images/lasso_regression2.png" /></p>
<p><img alt="SCAD" src="_images/SCAD.png" /></p>
<p>Another successful variable selection method is smoothly-clipped-absolute-deviation (SCAD)
[&#64;fan2001variable]. Its criterion function is</p>
<div class="math notranslate nohighlight">
\[
(2n)^{-1} (Y-X\beta)'(Y-X\beta) + \sum_{j=1}^d \rho_{\lambda}( |\beta_j| )
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\rho_{\lambda}^{\prime} (\theta) = \lambda \left\{ 1\{\theta\leq \lambda \} +
\frac{(a\lambda - \theta)_+}{(a-1)\lambda} \cdot 1 \{\theta &gt; \lambda\} \right\}
\]</div>
<p>for some <span class="math notranslate nohighlight">\(a&gt;2\)</span> and <span class="math notranslate nohighlight">\(\theta&gt;0\)</span>. This is a non-convex function, and &#64;fan2001variable establish the so-called
<em>oracle property</em>. An estimator boasting the oracle property can achieve variable selection consistency and
(pointwise) asymptotic normality simultaneously.</p>
<p>The follow-up <em>adaptive Lasso</em> [&#64;zou2006adaptive] also enjoys the oracle property.
Adaptive Lasso is a two step scheme: 1. First run a Lasso or ridge regression and save the estimator <span class="math notranslate nohighlight">\(\hat{\beta}^{(1)}\)</span>. 2. Solve</p>
<div class="math notranslate nohighlight">
\[
(2n)^{-1} (Y-X\beta)'(Y-X\beta) + \lambda \sum_{j=1}^d  w_j |\beta_j|
\]</div>
<p>where <span class="math notranslate nohighlight">\(w_j = 1 \bigg/ \left|\hat{\beta}_j^{(1)}\right|^a\)</span> and <span class="math notranslate nohighlight">\(a\geq 1\)</span> is a constant. (Common choice is <span class="math notranslate nohighlight">\(a = 1\)</span> or 2).</p>
<p>In R,  <code class="docutils literal notranslate"><span class="pre">glmnet</span></code> or <code class="docutils literal notranslate"><span class="pre">LARS</span></code> implements Lasso, and <code class="docutils literal notranslate"><span class="pre">ncvreg</span></code> carries out SCAD. Adaptive Lasso
can be done by setting the weight via the argument <code class="docutils literal notranslate"><span class="pre">penalty.factor</span></code> in <code class="docutils literal notranslate"><span class="pre">glmnet</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">&lt;-</span> <span class="m">40</span>
<span class="n">p</span> <span class="o">&lt;-</span> <span class="m">50</span>
<span class="n">b0</span> <span class="o">&lt;-</span> <span class="nf">c</span><span class="p">(</span><span class="nf">rep</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="m">10</span><span class="p">),</span> <span class="nf">rep</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="n">p</span> <span class="o">-</span> <span class="m">10</span><span class="p">))</span>
<span class="n">x</span> <span class="o">&lt;-</span> <span class="nf">matrix</span><span class="p">(</span><span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">p</span><span class="p">),</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
<span class="n">y</span> <span class="o">&lt;-</span> <span class="n">x</span> <span class="o">%*%</span> <span class="n">b0</span> <span class="o">+</span> <span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="n">ols</span> <span class="o">&lt;-</span> <span class="n">MASS</span><span class="o">::</span><span class="nf">ginv</span><span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">x</span><span class="p">)</span> <span class="o">%*%</span> <span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">y</span><span class="p">)</span> <span class="c1"># OLS</span>
<span class="c1"># Implement Lasso by glmnet</span>
<span class="n">cv_lasso</span> <span class="o">&lt;-</span> <span class="n">glmnet</span><span class="o">::</span><span class="nf">cv.glmnet</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">lasso_result</span> <span class="o">&lt;-</span> <span class="n">glmnet</span><span class="o">::</span><span class="nf">glmnet</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lambda</span> <span class="o">=</span> <span class="n">cv_lasso</span><span class="o">$</span><span class="n">lambda.min</span><span class="p">)</span>

<span class="c1"># Get weights</span>
<span class="n">b_temp</span> <span class="o">&lt;-</span> <span class="nf">as.numeric</span><span class="p">(</span><span class="n">lasso_result</span><span class="o">$</span><span class="n">beta</span><span class="p">)</span>
<span class="n">b_temp</span><span class="p">[</span><span class="n">b_temp</span> <span class="o">==</span> <span class="m">0</span><span class="p">]</span> <span class="o">&lt;-</span> <span class="m">1e-8</span>
<span class="n">w</span> <span class="o">&lt;-</span> <span class="m">1</span> <span class="o">/</span> <span class="nf">abs</span><span class="p">(</span><span class="n">b_temp</span><span class="p">)</span> <span class="c1"># Let gamma = 1</span>

<span class="c1"># Implement Adaptive Lasso by glmnet</span>
<span class="n">cv_alasso</span> <span class="o">&lt;-</span> <span class="n">glmnet</span><span class="o">::</span><span class="nf">cv.glmnet</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">penalty.factor</span> <span class="o">=</span> <span class="n">w</span><span class="p">)</span>
<span class="n">alasso_result</span> <span class="o">&lt;-</span>
  <span class="n">glmnet</span><span class="o">::</span><span class="nf">glmnet</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">penalty.factor</span> <span class="o">=</span> <span class="n">w</span><span class="p">,</span> <span class="n">lambda</span> <span class="o">=</span> <span class="n">cv_alasso</span><span class="o">$</span><span class="n">lambda.min</span><span class="p">)</span>

<span class="nf">plot</span><span class="p">(</span><span class="n">b0</span><span class="p">,</span> <span class="n">ylim</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">-0.8</span><span class="p">,</span> <span class="m">1.5</span><span class="p">),</span> <span class="n">pch</span> <span class="o">=</span> <span class="m">4</span><span class="p">,</span> <span class="n">xlab</span> <span class="o">=</span> <span class="s">&quot;&quot;</span><span class="p">,</span> <span class="n">ylab</span> <span class="o">=</span> <span class="s">&quot;coefficient&quot;</span><span class="p">)</span>
<span class="nf">points</span><span class="p">(</span><span class="n">lasso_result</span><span class="o">$</span><span class="n">beta</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;red&quot;</span><span class="p">,</span> <span class="n">pch</span> <span class="o">=</span> <span class="m">6</span><span class="p">)</span>
<span class="nf">points</span><span class="p">(</span><span class="n">alasso_result</span><span class="o">$</span><span class="n">beta</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;blue&quot;</span><span class="p">,</span> <span class="n">pch</span> <span class="o">=</span> <span class="m">5</span><span class="p">)</span>
<span class="nf">points</span><span class="p">(</span><span class="n">ols</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;green&quot;</span><span class="p">,</span> <span class="n">pch</span> <span class="o">=</span> <span class="m">3</span><span class="p">)</span>

<span class="c1"># out of sample prediction</span>
<span class="n">x_new</span> <span class="o">&lt;-</span> <span class="nf">matrix</span><span class="p">(</span><span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">p</span><span class="p">),</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
<span class="n">y_new</span> <span class="o">&lt;-</span> <span class="n">x_new</span> <span class="o">%*%</span> <span class="n">b0</span> <span class="o">+</span> <span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">lasso_msfe</span> <span class="o">&lt;-</span> <span class="p">(</span><span class="n">y_new</span> <span class="o">-</span> <span class="nf">predict</span><span class="p">(</span><span class="n">lasso_result</span><span class="p">,</span> <span class="n">newx</span> <span class="o">=</span> <span class="n">x_new</span><span class="p">))</span> <span class="o">%&gt;%</span> <span class="nf">var</span><span class="p">()</span>
<span class="n">alasso_msfe</span> <span class="o">&lt;-</span> <span class="p">(</span><span class="n">y_new</span> <span class="o">-</span> <span class="nf">predict</span><span class="p">(</span><span class="n">alasso_result</span><span class="p">,</span> <span class="n">newx</span> <span class="o">=</span> <span class="n">x_new</span><span class="p">))</span> <span class="o">%&gt;%</span> <span class="nf">var</span><span class="p">()</span>
<span class="n">ols_msfe</span> <span class="o">&lt;-</span> <span class="p">(</span><span class="n">y_new</span> <span class="o">-</span> <span class="n">x_new</span> <span class="o">%*%</span> <span class="n">ols</span><span class="p">)</span> <span class="o">%&gt;%</span> <span class="nf">var</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="n">lasso_msfe</span><span class="p">,</span> <span class="n">alasso_msfe</span><span class="p">,</span> <span class="n">ols_msfe</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>We can DIY Lasso by <code class="docutils literal notranslate"><span class="pre">CVXR</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">CVXR</span><span class="p">)</span>

<span class="n">lambda</span> <span class="o">&lt;-</span> <span class="m">2</span> <span class="o">*</span> <span class="n">cv_lasso</span><span class="o">$</span><span class="n">lambda.min</span> <span class="c1"># tuning parameter</span>

<span class="c1"># CVXR for Lasso</span>
<span class="n">beta_cvxr</span> <span class="o">&lt;-</span> <span class="nf">Variable</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
<span class="n">obj</span> <span class="o">&lt;-</span> <span class="nf">sum_squares</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">x</span> <span class="o">%*%</span> <span class="n">beta_cvxr</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="m">2</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span> <span class="o">+</span> <span class="n">lambda</span> <span class="o">*</span> <span class="nf">p_norm</span><span class="p">(</span><span class="n">beta_cvxr</span><span class="p">,</span> <span class="m">1</span><span class="p">)</span>
<span class="n">prob</span> <span class="o">&lt;-</span> <span class="nf">Problem</span><span class="p">(</span><span class="nf">Minimize</span><span class="p">(</span><span class="n">obj</span><span class="p">))</span>
<span class="n">lasso_cvxr</span> <span class="o">&lt;-</span> <span class="nf">solve</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
<span class="n">beta_cvxr_hat</span> <span class="o">&lt;-</span> <span class="n">lasso_cvxr</span><span class="o">$</span><span class="nf">getValue</span><span class="p">(</span><span class="n">beta_cvxr</span><span class="p">)</span> <span class="o">%&gt;%</span> <span class="nf">as.vector</span><span class="p">()</span> <span class="o">%&gt;%</span> <span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>More methods are available if prediction of the response variables is the sole purpose of the regression.
An intuitive one is called <em>stagewise forward selection</em>.
We start from an empty model. Given many candidate <span class="math notranslate nohighlight">\(x_j\)</span>, in each round we add the regressor that can
produce the biggest <span class="math notranslate nohighlight">\(R^2\)</span>. This method is similar to the idea of <span class="math notranslate nohighlight">\(L_2\)</span> componentwise boosting,
which does not adjust the coefficients fitted earlier.</p>
</div>
<div class="section" id="shrinkage-estimation-in-econometrics">
<h2>Shrinkage Estimation in Econometrics<a class="headerlink" href="#shrinkage-estimation-in-econometrics" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>&#64;su2016identifying: use shrinkage estimation for classification</p></li>
<li><p>&#64;shi2016estimation: convergence rate of GMM Lasso</p></li>
<li><p>&#64;lee2018: Lasso and adaptive Lasso in predictive regression</p></li>
<li><p>&#64;shi2019forward: forward selection</p></li>
<li><p>&#64;shi2020high: latent group in forecast combination</p></li>
</ul>
</div>
<div class="section" id="empirical-applications">
<h2>Empirical Applications<a class="headerlink" href="#empirical-applications" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>&#64;lehrer2017box: movie box office</p></li>
<li><p>&#64;feng2019taming: factor zoo, compare machine learning methods</p></li>
<li><p>&#64;chinco2017sparse: financial market, Lasso prediction</p></li>
</ul>
</div>
<div class="section" id="reading">
<h2>Reading<a class="headerlink" href="#reading" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Efron and Hastie: Ch. 16</p></li>
<li><p>&#64;athey2018impact</p></li>
</ul>
</div>
<div class="section" id="appendix">
<h2>Appendix<a class="headerlink" href="#appendix" title="Permalink to this headline">¶</a></h2>
<p>Suppose <span class="math notranslate nohighlight">\(y_i = x_i' \beta_0 + e_i\)</span>, where <span class="math notranslate nohighlight">\(e_i\)</span> is independent of <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(\mathrm{var}[e_i] = \sigma^2\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\min_{\beta} E[ (y_i - x_i' \beta)^2 ] = E[ (y_i - x_i' \beta_0)^2 ] = E[ e_i^2 ] = \sigma^2.
\]</div>
<p>This is the minimal error that can be achieved in the population.</p>
<p>In reality, we have a sample <span class="math notranslate nohighlight">\((y_i, x_i)\)</span> of <span class="math notranslate nohighlight">\(n\)</span> observations, and we estimate <span class="math notranslate nohighlight">\(\beta\)</span> by the OLS estimator <span class="math notranslate nohighlight">\(\hat{\beta} = (X'X)^{-1}X'y\)</span>.
The expectation of the SSR is</p>
<div class="math notranslate nohighlight">
\[
E\left[ \frac{1}{n} \sum_{i=1}^n (y_i - x_i' \hat{\beta})^2  \right]
= \frac{1}{n}  E\left[ e'(I_n - X(X'X)^{-1}X )e  \right]=  \frac{\sigma^2}{n}(n-p) = \sigma^2\left( 1 - \frac{p}{n} \right)
&lt; \sigma^2
\]</div>
<p>Asymptotically, if <span class="math notranslate nohighlight">\(p/n \to 0\)</span>, the two risks converge. Otherwise if <span class="math notranslate nohighlight">\(p/n \to c\)</span>, the expected SSR is strictly
smaller than the minimal population risk. The model is overfitted.</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            kernelName: "ir",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="04-optimization.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Numerical Optimization</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="06-ML2.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Prediction-Oriented Algorithms</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Zhentao Shi<br/>
    
        &copy; Copyright 2022.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>