---
title: "Numerical Optimization"
author: Zhentao Shi
date: "February 28, 2018"
bibliography: book.bib
biblio-style: apalike
link-citations: yes
fontsize: 11pt
urlcolor: blue
output:
  pdf_document: default
---

# Numerical Optimization

Optimization is the key step to carry out econometric extremum estimation.
A general optimization problem is formulated as
$$\min_{\theta \in \Theta } f(\theta) \,\, \mbox{ s.t. }  g(\theta) = 0, h(\theta) \leq 0,$$
where $f(\cdot)$ is a criterion function, $g(\theta) = 0$ is an equality constraint,
and $h(\theta)\leq 0$ is an inequality constraint.

Most established numerical optimization algorithms aim at finding a local minimum.
However, there is no guarantee to locate the global minimum when multiple local minima exist.

Optimization without the equality and/or inequality constraints is called
an *unconstrained* problem; otherwise it is called a *constrained* problem.
The constraints can be incorporated into the criterion function via Lagrangian.

## Methods

There are many optimization algorithms in the field of operational research;
they are variants of a small handful of main principles.

The fundamental idea for twice-differentiable objective function is the Newton's method.
A necessary condition for optimization is
$s(\theta) = \partial f(\theta) / \partial \theta = 0$.

At an initial trial value $\theta_0$, if $s(\theta_0) \neq 0$, the research is updated by 
$$
\theta_{t+1} = \theta_{t} -  \left( H(\theta_t)  \right)^{-1}  s(\theta_t)
$$
for $t=0,1,\cdots$ where
$H(\theta) = \frac{ \partial s(\theta )}{ \partial \theta}$
is the Hessian matrix.
The algorithm iterates
until $|\theta_{t+1} -\theta_{t}| < \epsilon$ (absolute criterion) and/or
$|\theta_{t+1} -\theta_{t}|/|\theta_{t}| < \epsilon$ (relative criterion), where
$\epsilon$ is a small positive number chosen as a tolerance level.



**Newton's Method.** Newton's method seeks the solution to
$s(\theta) = 0$. Recall that the first-order condition is a necessary condition but not a sufficient
condition. We still need to verify the second-order condition to identify
whether a root to $s(\theta)$ is associated to a minimizer or a maximizer,
and we compare the values of the mimima to decide a
global minimum.

It is clear that Newton's method requires 
computation of the gradient $s(\theta)$ and the Hessian $H(\theta)$.
Newton's method converges at quadratic rate, which is fast.

**Quasi-Newton Method.** The most well-known quasi-Newton algorithm is  
[BFGS](http://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm).
It avoids explicit calculation of the computationally expensive Hessian matrix. Instead, starting from an initial (inverse)
Hessian, it updates the Hessian by an explicit formula motivated from the idea of quadratic approximation.

**Derivative-Free Method.** [Nelder-Mead](http://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method)
is a simplex method. It searches a local minimum by reflection, expansion and contraction.

## Implementation

R's optimization infrastructure has been constantly improving.
[R Optimization Task View](http://cran.r-project.org/web/views/Optimization.html)
gives a survey of the available CRAN packages.

For general-purpose nonlinear optimization, the package
[`optimx`](http://cran.r-project.org/web/packages/optimx/index.html) [@nash2014best]
effectively replaces R's default optimization commands. `optimx` provides a unified
interface for various widely-used optimization algorithms. Moreover,
it facilitates comparison among optimization algorithms.

**Example**

We use `optimx` to solve pseudo Poisson maximum likelihood estimation (PPML).
If $y_i$ is a continuous random variable, it obviously does not follow a Poisson
distribution, whose support consists of non-negative integers. However, if the conditional mean model
$$E[y_i | x_i] = \exp( x_i' \beta),$$
is satisfied, we can still use the Poisson regression to obtain a consistent
estimator of the parameter $\beta$ even if $y_i$ does not follow a conditional
Poisson distribution.

If $Z$ follows a Poisson distribution with mean $\lambda$, the probability masss function
$$
\Pr(Z = k) = \frac{\mathrm{e}^{-\lambda} \lambda^k}{k!}, \mbox{ for }k=0,1,2,\ldots,
$$
so that
$$
\log \Pr(Y = y | x) =  -\exp(x'\beta) + y\cdot x'\beta - \log k!
$$
Since the last term is irrelevant to the parameter, the
log-likelihood function is
$$
\ell(\beta) = \log \Pr( \mathbf{y} | \mathbf{x};\beta ) =
-\sum_{i=1}^n \exp(x_i'\beta) + \sum_{i=1}^n y_i x_i'\beta.
$$
In addition, it is easy to write the gradient
$$
s(\beta) =\frac{\partial \ell(\beta)}{\partial \beta} =
-\sum_{i=1}^n \exp(x_i'\beta)x_i + \sum_{i=1}^n y_i x_i.
$$
and verify that the Hessian
$$
H(\beta) = \frac{\partial^2 \ell(\beta)}{\partial \beta \partial \beta'} =
-\sum_{i=1}^n \exp(x_i'\beta)x_i x_i' - \sum_{i=1}^n \exp(x_i'\beta)
$$
is negative definite. Therefore, $\ell(\beta)$ is strictly concave
in $\beta$.

In economics we have utility maximization and cost minimization.
In statistics we have maximum likelihood estimation and minimal least squared estimation.
In operational reserach, the default optimization is minimization, not maximization.
To follow this convention, here we formulate the *negative* log-likelihood.

```{r,cache=TRUE}
# Poisson likelihood
poisson.loglik = function( b ) {
  b = as.matrix( b )
  lambda =  exp( X %*% b )
  ell = -sum( -lambda + y *  log(lambda) )
  return(ell)
}
```

To implement optimization in `R`, it is recommended to write the criterion as a
function of the parameter. Data can be fed inside or outside of the function.
If the data is provided as additional arguments, these arguments must be explicit.
(In constrast, in `Matlab` the parameter must be the sole argument for the function to be
optimized, and data can only be injected through a nested function.)

```{r,cache=TRUE,tidy=TRUE,message=FALSE,warning=FALSE}
#implement both BFGS and Nelder-Mead for comparison.

library(AER)
library(numDeriv)
library(optimx)

## prepare the data
data("RecreationDemand")
y =  RecreationDemand$trips
X =  with(RecreationDemand, cbind(1, income) )

## estimation
b.init =  c(0,1)  # initial value
b.hat = optimx( b.init, poisson.loglik,
                method = c("BFGS", "Nelder-Mead"),
                 control = list(reltol = 1e-7,
                                abstol = 1e-7)  )
print( b.hat )
```

Given the conditional mean model, nonlinear least squares (NLS) is also consistent in theory.
NLS minimizes
$$
\sum_{i=1}^n (y_i - \exp(x_i \beta))^2
$$
A natural question is: why do we prefer PPML to NLS?
My argument is that, PPML's optimization for the linear index is globally convex, while NLS is not.
It implies that the numerical optimization of PPML is easier and more robust than that of NLS. I leave the derivation of the non-convexity of NLS
as an exercise.


In practice no algorithm suits all problems. Simulation, where the true parameter is known,
  is helpful to check the accuracy of one's optimization routine before applying to an empirical problem,
  where the true parameter is unknown.
Contour plot is a useful tool to visualize the function surface/manifold in a low dimension.

**Example**

```{r,cache=TRUE,tidy=TRUE}
x.grid = seq(0, 1.8, 0.02)
x.length = length(x.grid)
y.grid = seq(-.5, .2, 0.01)
y.length = length(y.grid)

z.contour = matrix(0, nrow = x.length, ncol = y.length)

for (i in 1:x.length){
  for (j in 1:y.length){
    z.contour[i,j] = poisson.loglik( c( x.grid[i], y.grid[j] )  )
  }
}

contour( x.grid,  y.grid, z.contour, 20)
```

For problems that demand more accuracy,  third-party standalone solvers can be
invoked via interfaces to `R`.
For example, we can access [`NLopt`](http://ab-initio.mit.edu/wiki/index.php/NLopt_Installation)
through the packages [`nloptr`](http://cran.r-project.org/web/packages/nloptr/index.html).
However, standalone solvers usually have to be compiled and configured.
These steps are often not as straightforward as installing commercial Windows software.

`NLopt` offers an [extensive list of algorithms](http://ab-initio.mit.edu/wiki/index.php/NLopt_Algorithms#SLSQP).

**Example**

We first carry out the Nelder-Mead algorithm in NLOPT.

```{r,cache=TRUE,tidy=TRUE}
library(nloptr)
## optimization with NLoptr

opts = list("algorithm"="NLOPT_LN_NELDERMEAD",
            "xtol_rel"=1.0e-7,
            maxeval = 500
)

res_NM = nloptr( x0=b.init,
                 eval_f=poisson.loglik,
                 opts=opts)
print( res_NM )

# "SLSQP" is indeed the BFGS algorithm in NLopt,
# though "BFGS" doesn't appear in the name
opts = list("algorithm"="NLOPT_LD_SLSQP","xtol_rel"=1.0e-7)
```


To invoke BFGS in NLOPT, we must code up the gradient $s(\beta)$,
as in the function `poisson.log.grad()` below.


```{r,cache=TRUE,tidy=TRUE}
poisson.loglik.grad = function( b ) {
  b = as.matrix( b )
  lambda =  exp( X %*% b )
  ell = -colSums( -as.vector(lambda) * X + y *  X )
  return(ell)
}
```
We compare the analytical gradient with the numerical gradient to make sure
the function is correct.

```{r,cache=TRUE,tidy=TRUE}
# check the numerical gradient and the analytical gradient
b = c(0,.5)
grad(poisson.loglik, b)
poisson.loglik.grad(b)
```

With the function of gradient, we are ready for BFGS.

```{r,cache=TRUE,tidy=TRUE}
res_BFGS = nloptr( x0=b.init,
                   eval_f=poisson.loglik,
                   eval_grad_f = poisson.loglik.grad,
                   opts=opts)
print( res_BFGS )
```

## Contrained Optimization

* `optimx` can handle simple box-constrained problems.
* `constrOptim` can handle linear constrained problems.
* Some algorithms in `nloptr`, for example, `NLOPT_LD_SLSQP`, can handle nonlinear constrained problems.
* `Rdonlp2` is an alternative for general nonlinear constrained problems. Rdonlp2
is a package offered by `Rmetric` project. It can be installed by
 `install.packages("Rdonlp2", repos="http://R-Forge.R-project.org")`

## Convex Optimization

If a function is convex in its argument, then a local minimum is a global minimum.
Convex optimization is particularly important in high-dimensional problems. The readers are
referred to @boyd2004convex for an accessible comprehensive treatment. They claim that
"convex optimization is technology; all other optimizations are arts." This is true to some extent.


**Example**

* linear regression model MLE
* Lasso [@su2016identifying]
* Relaxed empirical likelihood [@shi2016econometric].

A class of common convex optimization can be reliably implemented in `R`.
[`Rmosek`](http://rmosek.r-forge.r-project.org/) is an interface in `R` to access `Mosek`.
`Mosek` is a high-quality commercial solver dedicated to convex optimization.
It offers free academic licenses. (`Rtools` is a prerequisite to install `Rmosek`.)

```{r,eval=FALSE,tidy=TRUE}
require(Rmosek)
lo1 <- list()
lo1$sense <- "max"
lo1$c <- c(3,1,5,1)
lo1$A <- Matrix(c(3,1,2,0,
                  2,1,3,1,
                  0,2,0,3),
          nrow=3, byrow=TRUE, sparse=TRUE)
lo1$bc <- rbind(blc = c(30,15,-Inf),
                buc = c(30,Inf,25))
lo1$bx <- rbind(blx = c(0,0,0,0),
                bux = c(Inf,10,Inf,Inf))
r <- mosek(lo1)
```

A survey paper can be found here with [some recent development in convex optimization in econometrics](https://www.researchgate.net/publication/320805166_Convex_Programming_in_Econometrics). 


## References
